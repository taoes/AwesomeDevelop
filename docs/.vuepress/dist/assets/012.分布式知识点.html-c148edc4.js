import{_ as o,r,o as t,c,a as l,d as e,b as d,e as i}from"./app-d035ab8f.js";const n={},p=i('<h2 id="实现方案" tabindex="-1"><a class="header-anchor" href="#实现方案" aria-hidden="true">#</a> 实现方案</h2><ul><li>Nginx 七层负载均衡 LVS 四层负载均衡</li><li>F5 硬件负载均衡</li></ul><h2 id="分布式事务" tabindex="-1"><a class="header-anchor" href="#分布式事务" aria-hidden="true">#</a> 分布式事务</h2><p>事务的操作位于不同的节点上，需要保证事务的 ACID 特性。 eg: 下单和库存处于不同的节点上时候就需要分布式事务</p><ul><li><p>二阶段提交</p><ol><li>引入协调者，协调者询问参与者是否都执行成功，参与者返回 yes/no。</li><li>如果参数这都返回执行成功/失败，那么协调者发送提交/回滚指令。</li><li>存在的问题 <blockquote><ul><li>同步阻塞: 所有事务参与者在等待其它参与者响应的时候都处于同步阻塞等待状态，无法进行其它操作</li><li>单点问题: 协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。</li><li>数据不一致: 在提交阶段，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致</li><li>过于保守：任意一个节点失败就会导致整个事务失败，没有完善的容错机制。</li></ul></blockquote></li></ol></li><li><p>三阶段提交</p><ol><li>canCommit <blockquote><ol><li>协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。</li><li>响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No</li></ol></blockquote></li><li>preCommit</li><li>doCommit</li></ol></li><li><p>总结</p><ol><li>2PC 一致性好、可用性较低，3PC一致性较低、可用性高</li></ol></li></ul><h2 id="分布式锁的实现" tabindex="-1"><a class="header-anchor" href="#分布式锁的实现" aria-hidden="true">#</a> 分布式锁的实现</h2><ul><li>(单机) 互斥量或临界资源实现</li><li>DataBase 唯一索引： 但其不可重入，无重试机制、超时机制</li><li>zookeeper: 有序节点 <ol><li>创建一个锁目录 /lock；</li><li>当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点；</li><li>客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁</li><li>执行业务代码，完成后，删除对应的子节点</li></ol></li><li>Redis： <ol><li>SetNX + Expire</li><li>RedLock</li></ol><blockquote><ol><li>尝试从 N 个互相独立 Redis 实例获取锁；</li><li>计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，才认为获取锁成功；</li><li>如果获取锁失败，就到每个实例上释放锁</li></ol></blockquote></li></ul><h2 id="分布式算法" tabindex="-1"><a class="header-anchor" href="#分布式算法" aria-hidden="true">#</a> 分布式算法</h2><ul><li><p>负载均衡</p><ol><li>（加权）轮询</li><li>（加权） 随机</li><li>源地址HASH算法(一致性HASH算法，哈希环)</li><li>最少连接算法</li></ol></li><li><p>Nginx加权随机的实践</p><ol><li>设定范围 1-10-100 然后生成数看看落在那个范围内</li><li>按照权重总和生成多个IP，然后依旧随机</li></ol></li><li><p>Nginx轮询算法(RoundRobin)</p><ol><li>和 随机算法类似</li></ol></li><li><p>分布式ID生成器</p><ol><li>基于HA数据库实现</li><li>基于Redis 自动增长ID实现</li><li>雪花(snowflake)算法，8个字节，第1位不用，40位存储时间戳，10位存储工作机器ID(前5位机房，后5位设备)，12位序列号 支持1ms内生成4096个ID，在不够的情况下，可以缩小机器ID的范围( 时钟回拨的问题，当前时间比上次时间小)</li></ol></li></ul><h2 id="分布式事务tcc-sentinel" tabindex="-1"><a class="header-anchor" href="#分布式事务tcc-sentinel" aria-hidden="true">#</a> 分布式事务TCC (Sentinel)</h2><ol><li>TCC 也是拓展至2PC，也是一种分布式事务的方案</li><li>Seata: TC --&gt; TM: 子事务向TC注册相同组ID的事务，TC发现有事务执行失败，则协调全部回滚</li><li>TCC的空回滚： TM 在 TRY的时候网络超时，造成TC没有收到注册，然后TM认为事务失败，向TC发起Cancel,此时TC会进行空回滚</li><li>TCC的防悬挂: TM 在发起TRY阶段的时候网路卡顿，TM任务超时，然后发起Cancel成功，此时TC再也无法接收到TRY的确认或者去取消</li><li>TCC需要考虑幂等性，防止因为网络延时或者补偿的时候造成问题，另外在二阶段提交之前，所有的事务都要等待，因此需要锁的粒度非常小</li></ol>',11),h={href:"https://segmentfault.com/a/1190000015612188?utm_source=sf-similar-article",target:"_blank",rel:"noopener noreferrer"},s=i('<h2 id="分布式概念" tabindex="-1"><a class="header-anchor" href="#分布式概念" aria-hidden="true">#</a> 分布式概念</h2><h3 id="_1、一致性协议" tabindex="-1"><a class="header-anchor" href="#_1、一致性协议" aria-hidden="true">#</a> 1、一致性协议</h3><h4 id="_1-1、paxos" tabindex="-1"><a class="header-anchor" href="#_1-1、paxos" aria-hidden="true">#</a> 1.1、Paxos</h4><p>角色: 提议者, 接受者 &amp; 记录者</p><ul><li><p>prepare 阶段：</p><blockquote><ol><li>Proposer 选择一个提案编号 n，然后向acceptor的某个超过半数的子成员发送编号为n的 prepare 请求；</li></ol></blockquote><blockquote><ol start="2"><li>Acceptor 收到 prepare 消息后，如果提案的编号n大于该acceptor已经回复的所有 prepare 请求的编号，则 Acceptor 将自己上次已经批准的最大编号提案回复给 Proposer，并承诺不再回复小于 n 的提案；</li></ol></blockquote></li><li><p>acceptor阶段：</p><blockquote><ol><li>当一个 Proposer 收到了半数以上的 Acceptors 对 prepare 的回复后，就进入批准阶段。它要向回复 prepare 请求的 Acceptors 发送 accept 请求，包括编号 n 和根据 prepare阶段 决定的 value。这里的value是所有响应中编号最大的提案的value（如果根据 prepare 没有已经接受的 value，那么它可以自由决定 value）。</li></ol></blockquote><blockquote><ol start="2"><li>在不违背自己向其他 Proposer 的承诺的前提下，Acceptor 收到 accept 请求后即接受这个请求。即如果acceptor收到这个针对n提案的accep请求，只要改acceptor尚未对编号大于n的prepare请求做出过响应，它就可以通过这个提案。</li></ol></blockquote></li></ul><h4 id="_1-2、raft-协议-共识算法-etcd" tabindex="-1"><a class="header-anchor" href="#_1-2、raft-协议-共识算法-etcd" aria-hidden="true">#</a> 1.2、Raft 协议(共识算法,etcd)</h4><p>主要用于选举，有三种角色：主节点，从节点以及参与投票的节点。初始阶段，所有的阶段都是从节点，每个阶段随机休眠不同的时间，先醒的节点依次变为参与节点</p><ol><li>参与节点发起投票¸其他收到投票请求且还未投票的Follower节点会向发起者投票</li><li>当得票数超过了集群节点数量的一半，该节点晋升为Leader节点。</li><li>Leader节点会立刻向其他节点发出通知，告诉大家自己才是老大。收到通知的节点全部变为Follower，并且各自的计时器清零。</li><li>Leader节点需要每隔一段时间向集群其他节点发送心跳通知，表明你们的老大还活着。</li><li>一旦Leader节点挂掉，发不出通知，那么计时达到了超时时间的Follower节点会转变为Candidate节点，发起选主投票</li></ol><ul><li>同步过程 <ol><li>由客户端提交数据到Leader节点</li><li>由Leader节点把数据复制到集群内所有的Follower节点。如果一次复制失败，会不断进行重试。</li><li>Follower节点们接收到复制的数据，会反馈给Leader节点。</li><li>如果Leader节点接收到超过半数的Follower反馈，表明复制成功。于是提交自己的数据，并通知客户端数据提交成功。</li><li>由Leader节点通知集群内所有的Follower节点提交数据，从而完成数据同步流程</li></ol></li></ul><h4 id="_1-3、zap-协议-zk原子消息广播协议-zk-实现" tabindex="-1"><a class="header-anchor" href="#_1-3、zap-协议-zk原子消息广播协议-zk-实现" aria-hidden="true">#</a> 1.3、ZAP 协议(ZK原子消息广播协议，ZK 实现)</h4><blockquote><p>所有的事务请求(会改变服务器数据状态的请求，如修改节点数据、删除节点等)必须由一个全局唯一的服务器来协调处理，该服务器被称为Leader，而剩余的其他服务器被称为Follower。</p></blockquote><ul><li>崩溃恢复模式</li></ul><blockquote><p>当Zookeeper集群初始化时，或Leader故障宕机时，ZAB协议就会进入崩溃恢复模式，并选举出新的Leader。当新的Leader选举出来后，并且集群中已经有过半的节点与Leader完成了数据同步，ZAB协议就会退出崩溃恢复模式，转而进入消息广播模式。一个节点要想成为Leader，必须获得集群中过半节点的支持。</p></blockquote><ul><li>消息广播模式</li></ul><blockquote><p>Leader负责将一个客户端的事务请求转换成一个事务Proposal(提议)，并将该Proposal分发给集群中的所有Follower。然后Leader等待所有Follower的反馈结果，一旦有超过半数的Follower做出了正确的反馈后，Leader就会向所有的Follower再次发送Commit消息，要求将前一个Proposal提交。</p></blockquote><ul><li>为什么建议是奇数个？</li></ul><blockquote><p>所谓的zookeeper容错是指，当宕掉几个zookeeper服务器之后，剩下的个数必须大于宕掉的个数，也就是剩下的服务数必须大于n/2，zookeeper才可以继续使用，无论奇偶数都可以选举leader。</p></blockquote><p>等待完善</p><h3 id="_2、相关理论" tabindex="-1"><a class="header-anchor" href="#_2、相关理论" aria-hidden="true">#</a> 2、相关理论</h3><h4 id="_2-1、cap-理论" tabindex="-1"><a class="header-anchor" href="#_2-1、cap-理论" aria-hidden="true">#</a> 2.1、CAP  理论</h4><p>CAP: 一致性、可用性与分区容错性。 CAP中三者只能实现2个，因为P是肯定要有的，所以只能实现CP或者AP。为了保证一致性（CP），不能访问未同步完成的节点，也就失去了部分可用性；为了保证可用性（AP），允许读取所有节点的数据，但是数据可能不一致</p><h4 id="_2-2、base-理论" tabindex="-1"><a class="header-anchor" href="#_2-2、base-理论" aria-hidden="true">#</a> 2.2、BASE 理论</h4><p>BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）。 基本可用: 可能会出现服务降级等；软状态: 如支付 可能会出现非最终的中间状态, 如支付中，支付查询中等； 最终一致性: 非CAP中的强一致性，这里的一致性值得最终一致性，允许此时短暂时间的内的数据不一致的文图</p><h2 id="其他" tabindex="-1"><a class="header-anchor" href="#其他" aria-hidden="true">#</a> 其他</h2><h3 id="概念区分" tabindex="-1"><a class="header-anchor" href="#概念区分" aria-hidden="true">#</a> 概念区分</h3><ul><li>集群： 注重物理布局</li><li>分布式: 注重工作方式</li><li>SOA: 面向服务的中心化分布式框架，ESB存在单点</li><li>微服务: 基于RestAPI的去中心化的分布式服务</li><li>分布式锁注重于互斥区资源的访问，分布式事务则在于ACID的实现</li></ul>',26);function u(_,b){const a=r("ExternalLinkIcon");return t(),c("div",null,[p,l("blockquote",null,[l("p",null,[e("参考文章 "),l("a",h,[e("https://segmentfault.com/a/1190000015612188?utm_source=sf-similar-article"),d(a)])])]),s])}const k=o(n,[["render",u],["__file","012.分布式知识点.html.vue"]]);export{k as default};
